---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---
The first step involves setting the working directory and importing the required libraries
```{r}
#install.packages("ff")
LibImport <- function(path)
{
  setwd(path)
  #library("ff")
  library(ggplot2)
  library(caret)
  library(rpart)
  library(randomForest)
  load.libraries <- c('data.table', 'testthat', 'gridExtra', 'corrplot', 'GGally', 'ggplot2', 'e1071', 'dplyr','Hmisc')
  install.lib <- load.libraries[!load.libraries %in% installed.packages()]
  for(libs in install.lib) install.packages(libs, dependences = TRUE)
  sapply(load.libraries, require, character = TRUE)
}
```

Calling the Library Import Function: - 
```{r}
path.var = "C:/Users/LEON/Desktop/Ds Udemy/House_Pred"
LibImport(path.var)
```

Creating a function to count features with missing values and to plot a visual of them.
```{r}
PlotMissing <- function(full.dat)
{
    num_na <- sort(colSums(is.na(full.dat)), decreasing = TRUE)
    num_na <- num_na[num_na>0]
    
    ggplot()+ geom_bar(aes(x=reorder(names(num_na),num_na), y= num_na),stat = 'identity')+xlab("Variables with Missing Values")+ylab("Number of missing values")+coord_flip()
}
```


```{r}
test_data <- read.csv("test.csv",stringsAsFactors = FALSE)
train_data <- read.csv("train.csv",stringsAsFactors = FALSE)
summary(test_data)

target_variable <- train_data$SalePrice

full <- rbind(train_data[,1:80],test_data)

PlotMissing(full)
```

The next step is to explore these features and determine the percentage of missing values for each of them to decide whether to drop them or impute the missing values.
```{r}
summary(full$PoolQC)
summary(full$MiscFeature)
summary(full$Alley)
summary(full$Fence)
summary(full$FireplaceQu)
summary(full$LotFrontage)
feat_excluded <- c("PoolQC","MiscFeature","Alley","Fence","FireplaceQu")
```

```{r}
ExtractColTypes <- function(train_data)
{
  cat_var <- names(train_data)[which(sapply(train_data, is.factor))]
  cat_car <- c(cat_var, 'BedroomAbvGr', 'HalfBath', ' KitchenAbvGr','BsmtFullBath', 'BsmtHalfBath', 'MSSubClass')
  numeric_var <- names(train_data)[which(sapply(train_data, is.numeric))]

  return(list(v1=cat_var,v2=cat_car,v3=numeric_var))
  }

list_types <- ExtractColTypes(train_data)

cat_var <- list_types$v1
cat_car <- list_types$v2
numeric_var <- list_types$v3
```


The next step involves plotting a correlation matrix for the numeric variables. We create a function for reusability.
```{r}
PlotCorMat <- function(df,num_var)
{
cor_matx <- round(cor(na.omit(df[,names(df)%in%num_var])),2)

corrplot(corr = cor_matx, method = "circle",type='lower',title = "Correlation plot of Numeric Variables",addCoefasPercent=TRUE,order = "FPC",tl.cex = 0.6,tl.srt = 45,tl.col ='black')
}
```

Creating the Correlation Map for the entire dataset excluding the Target Variable.
```{r}
PlotCorMat(full,numeric_var)
```

Calculating the percentage of missing values for each feature having NA's
```{r}
list_na_vars = c('PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage','GarageYrBlt','GarageQual','GarageFinish','GarageCond','GarageType')
na_values <- as.list(NULL)
na_values <- colSums(is.na(full[,list_na_vars]))
na_percs <- lapply(na_values, FUN = function(x){((x/2919)*100)})
df.na_percs <- as.data.frame(na_percs)
na_values <- as.data.frame(na_values)
na_values
df.na_percs
```


Imputing the missing values for the missing values or deciding to drop or keep features.
```{r}
summary(full$LotFrontage)
#full$LotFrontage[which(is.na(full$LotFrontage))] <- 68
summary(full$LotFrontage)

summary(full$FireplaceQu)
summary(full$Fireplaces)
ggplot(full, aes(x=full$FireplaceQu, y=full$Fireplaces)) + stat_summary(fun.y ="length", geom="bar")

summary(full$GarageYrBlt)
ggplot(full, aes(x=full$GarageType, y=full$GarageCars)) + stat_summary(fun.y ="length", geom="bar")

full$PoolQC[is.na(full$PoolQC)] <- 'None'

library(Amelia)
library(mice)
imp.df <- full
imp.LotFrontage <- mice(imp.df, m=1, method='cart', printFlag=FALSE)

sub.df <- imp.LotFrontage$imp$LotFrontage
sub.df$Id <- rownames(sub.df)

i <-0
fr <- length(sub.df$`1`)
for(i in 1:fr)
{
    full$LotFrontage[full$Id == sub.df$Id[i]] <- sub.df$`1`[i]
}

summary(full$LotFrontage)
length(full$LotFrontage)

```


Imputaitons for features that have less than 5% of missing values. It did not make sense to drop these features and so Naive Rule was used for imputing values.
```{r}
full$GarageYrBlt[is.na(full$GarageYrBlt)] <- 3000
full$GarageQual[is.na(full$GarageQual)] <- 'None'
full$GarageFinish[is.na(full$GarageFinish)] <- 'None'
full$FireplaceQu[is.na(full$FireplaceQu)] <- 'None'
full$BsmtFinType2[is.na(full$BsmtFinType2)] <- 'None'
full$BsmtExposure[is.na(full$BsmtExposure)] <- 'None'
full$BsmtQual[is.na(full$BsmtQual)] <- 'None'
full$BsmtFinType1[is.na(full$BsmtFinType1)] <- 'None'
full$BsmtCond[is.na(full$BsmtCond)] <- 'None'
full$MasVnrType[is.na(full$MasVnrType)] <- 'None'
full$MasVnrArea[is.na(full$MasVnrArea)] <- 0.0
full$MSZoning[is.na(full$MSZoning)] <- 'RL'
full$Electrical[is.na(full$Electrical)] <- 'SBrkr'
full$Utilities[is.na(full$Utilities)] <- 'AllPub'
full$Functional[is.na(full$Functional)] <- 'Typ'
full$BsmtHalfBath[is.na(full$BsmtHalfBath)] <- 0
full$BsmtFullBath[is.na(full$BsmtFullBath)] <- 0
full$TotalBsmtSF[is.na(full$TotalBsmtSF)] <- 1050
full$SaleType[is.na(full$SaleType)] <- 'WD'
full$KitchenQual[is.na(full$KitchenQual)] <- 'None'
full$GarageCars[is.na(full$GarageCars)] <- 0
full$GarageArea[is.na(full$GarageArea)] <- 473
full$Exterior2nd[is.na(full$Exterior2nd)] <- 'VinylSd'
full$Exterior1st[is.na(full$Exterior1st)] <- 'VinylSd'
full$BsmtUnfSF[is.na(full$BsmtUnfSF)] <- 560
full$BsmtFinSF2[is.na(full$BsmtFinSF2)] <- 49.58
full$BsmtFinSF1[is.na(full$BsmtFinSF1)] <- 441.4
```

Plotting correations for the training dataset features against the target variable
```{r}
cor_tr <- cbind(train_data[,names(train_data)%in%numeric_var],target_variable)

train_cor_mat <- round(cor(na.omit(cor_tr)),2)
train_cor_mat1 <- rcorr(as.matrix(cor_tr))

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

flat_cor_mat <- flattenCorrMatrix(train_cor_mat1$r, train_cor_mat1$P)
flat_cor_mat <- filter(flat_cor_mat,flat_cor_mat$cor>0.7)

corrplot(corr = train_cor_mat, method = "circle",type = 'lower',order = 'FPC',title = "Correlation plot of Training Numeric Variables and Target Variable",tl.col = "black",tl.cex = 0.6,tl.offset=0.5 ,tl.srt = 45,diag=TRUE)

flat_cor_mat
```

##########################################################################################################################


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Importing Additional Libraries
```{r, message=FALSE}

library(Amelia)
library(mice)
library(ggplot2)
library(lattice)

```


Deciding on which features to drop
```{r}
exclude <- c('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'GarageCond', 'GarageType')
include <- setdiff(names(full), exclude)
```


Fitting the data to a Random Forest Model to serve as a Benchmark
```{r}
#Identifying which feautures are char type to convert them to factors for th model
to_conv <- names(full)[which(sapply(full, is.character))]
to_not_conv <- names(full)[which(sapply(full, is.integer))]
full <- full %>% mutate_if(is.character, as.factor)

#Splitting the data back into training and test sets
train_new <- data.frame(cbind(full[1:1460,include], target_variable), stringsAsFactors = TRUE)
test_new <- data.frame(full[1461:2919, include], stringsAsFactors = TRUE)

#Fitting the Model to the Data
tree_model <- randomForest(formula = train_new$target_variable ~ . , data = train_new) 

#Generating predictions using the fitted Model
pred_values <- predict(tree_model, test_new, type = 'response')

#Combining the final predictions into a a dataframe along witht he top correlated features
final_preds <- data.frame(cbind(test_new$Id, test_new$OverallQual, test_new$GarageCars, test_new$GrLivArea, pred_values))
colnames(final_preds) <- c('House Id','OverallQual','GarageCars','GrLivArea','Predicted_Sale_Price')

#Plotting the variance of the Sale_Price predictions with the top correlated features
xyplot(x = final_preds$Predicted_Sale_Price ~ final_preds$OverallQual, data = final_preds)
xyplot(x = final_preds$Predicted_Sale_Price ~ final_preds$GarageCars, data = final_preds)
xyplot(x = final_preds$Predicted_Sale_Price ~ final_preds$GrLivArea, data = final_preds)
```


Writing the results to a final submission csv file
```{r}

setwd("C:/Users/LEON/Desktop/Ds Udemy/House_Pred")
final_submission <- data.frame(cbind(final_preds$`House Id`, final_preds$Predicted_Sale_Price))
colnames(final_submission) <- c('Id','SalePrice')
write.csv(final_submission , file = 'bvikram_submission.csv', row.names = FALSE)
```